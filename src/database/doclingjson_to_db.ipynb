{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from database_setup import *\n",
    "from model import *\n",
    "\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling_core.types.doc import DoclingDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_docs = sorted(glob.glob(\"/workspace/src/docling_out/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63869"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docling_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63869"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docling_dois = [x.split(\"/\")[-1].replace(\".json\", \"\").replace(\"$\", \"/\") for x in docling_docs]\n",
    "len(docling_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding titles\n",
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"cord19/fulltext/trec-covid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = DoclingDocument.load_from_json(\"/workspace/src/docling_out/10.1016$j.antiviral.2007.12.008.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oligonucleotide antiviral therapeutics: Antisense and RNA interference for highly pathogenic RNA viruses\n",
      "Abstract\n",
      "1. Introduction\n",
      "2. Antisense oligonucleotides (ASOs)\n",
      "3. RNA interference (RNAi)\n",
      "4. Antisense and siRNA as antiviral therapeutics\n",
      "4.1. Filoviruses\n",
      "4.2. Flaviviruses\n",
      "4.3. Arenaviruses\n",
      "4.4. Alphaviruses\n",
      "4.5. SARS-associated coronavirus (SARS Co-V)\n",
      "4.6. Influenza A\n",
      "5. Conclusion\n",
      "Acknowledgements\n",
      "References\n"
     ]
    }
   ],
   "source": [
    "# um Titel zu extrahieren\n",
    "\n",
    "for text in test_doc.texts:\n",
    "    #if \"Oligonucleotide antiviral therapeutics:\" in text.text:\n",
    "    if text.label == \"section_header\":\n",
    "        print(text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord19_doc = dataset.docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192509/192509 [00:54<00:00, 3561.85it/s]\n"
     ]
    }
   ],
   "source": [
    "titles = {}\n",
    "abstracts = {}\n",
    "for doc in tqdm(cord19_doc):\n",
    "    if doc.doi in docling_dois:\n",
    "        titles[doc.doi] = doc.title\n",
    "        abstracts[doc.doi] = doc.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_vals = dotenv_values(\"/workspace/src/.env\")\n",
    "session = setup_engine_session(db_vals['USER'], db_vals['PASSWORD'], db_vals['ADDRESS'], db_vals['PORT'], db_vals['DB'], echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel_extract_data(docling_docs[38860:], model, session, num_workers=8, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_document_title(doi, titles_dict, session):\n",
    "    session.query(Document).filter(Document.doi == doi).update(\n",
    "    {\n",
    "        Document.title: titles_dict[doi]\n",
    "    },\n",
    "    synchronize_session=False\n",
    "    )\n",
    "    try:\n",
    "        session.commit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error committing session: {e}\")\n",
    "        session.rollback()\n",
    "\n",
    "\n",
    "def update_document(doi, title_dict, abstract_dict, session):\n",
    "    session.query(Document).filter(Document.doi == doi).update(\n",
    "    {\n",
    "        Document.abstract: abstract_dict[doi],\n",
    "        Document.title: titles_dict[doi]\n",
    "    },\n",
    "    synchronize_session=False\n",
    "    )\n",
    "    try:\n",
    "        session.commit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error committing session: {e}\")\n",
    "        session.rollback()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parallel_update_data(data_dict, session, field=\"title\", num_workers=None, batch_size = 10):\n",
    "    # data_dict is {doi:data, …} for a given field, e.g.: {1001.3456: \"this is a title\", …} for title\n",
    "    data_list = [(a, data_dict, session) for a in list(data_dict.keys())]\n",
    "    with tqdm(total=len(data_list), desc=\"Overall Progress\") as pbar:\n",
    "        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "            for i in range(0, len(data_list), batch_size):\n",
    "                batch_paths = data_list[i:i + batch_size]\n",
    "\n",
    "                # Extract data in parallel for the current batch\n",
    "                with tqdm(total=len(batch_paths), desc=\"Extracting Data\", leave=False) as extract_pbar:\n",
    "  \n",
    "                    # Using list() to eagerly evaluate executor.map and catch exceptions\n",
    "                    for data in executor.map(update_document_title, batch_paths):\n",
    "                        if data is not None:\n",
    "                            if field == \"title\":\n",
    "                                update_document_title(data)\n",
    "                        extract_pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/63869 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'weakref.ReferenceType' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 244, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"/usr/lib/python3.10/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\nTypeError: cannot pickle 'weakref.ReferenceType' object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parallel_update_data(titles, session)\n",
      "Cell \u001b[0;32mIn[50], line 26\u001b[0m, in \u001b[0;36mparallel_update_data\u001b[0;34m(data_dict, session, field, num_workers, batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Extract data in parallel for the current batch\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(batch_paths), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting Data\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m extract_pbar:\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m     \u001b[39m# Using list() to eagerly evaluate executor.map and catch exceptions\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m executor\u001b[39m.\u001b[39mmap(update_document_title, batch_paths):\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m             \u001b[39mif\u001b[39;00m field \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[39m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[39m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[39mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    622\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    320\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:244\u001b[0m, in \u001b[0;36mQueue._feed\u001b[0;34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# serialize the data before acquiring the lock\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m obj \u001b[39m=\u001b[39m _ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj)\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m wacquire \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     send_bytes(obj)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'weakref.ReferenceType' object"
     ]
    }
   ],
   "source": [
    "parallel_update_data(titles, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_document_title('10.1001/amajethics.2020.344', titles, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63869/63869 [1:25:30<00:00, 12.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for doi in tqdm(titles.keys()):\n",
    "    update_document_title(doi, titles, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
