{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# We save the model to a local mounted directory\n",
    "os.environ['HF_HOME'] = '/workspace/llm_models'\n",
    "\n",
    "modality = \"passage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "from umbrela.gpt_judge import GPTJudge\n",
    "from umbrela.osllm_judge import OSLLMJudge \n",
    "import torch\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing.utils import parallel_process_topics, single_process_topics, load_vectorstore, load_pool_documents\n",
    "\n",
    "db_vals = dotenv_values(\"/workspace/src/.env\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/preprocessing/utils.py:87: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings(model=embedding_model)\n",
      "/workspace/src/preprocessing/utils.py:88: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "passage_pool = json.load(open(\"/workspace/src/data/passage_pool.json\", \"r\"))\n",
    "table_pool = json.load(open(\"/workspace/src/data/table_pool.json\", \"r\"))\n",
    "\n",
    "passage_pool_for_rel = json.load(open(\"/workspace/src/data/passage_pool_for_rel.json\", \"r\"))\n",
    "\n",
    "topics = pickle.load(open(\"/workspace/src/data/topics.pkl\", \"rb\"))\n",
    "vectorstore = load_vectorstore(\"/workspace/src/preprocessing/vectorstores/chromadb_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modality == 'table':\n",
    "    pool_documents = load_pool_documents(table_pool, vectorstore)\n",
    "else:\n",
    "    pool_documents = load_pool_documents(passage_pool, vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"mistralai/Mistral-Small-Instruct-2409\",\n",
    "    \"microsoft/phi-4\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"tiiuae/Falcon3-7B-Instruct\",\n",
    "               \n",
    "               \"meta-llama/Llama-3.2-3B-Instruct\",  \n",
    "               \"Qwen/Qwen2.5-14B-Instruct\", \n",
    "                \n",
    "               \"google/gemma-2-9b-it\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gauge execution time\n",
    "start_time = time.time()    \n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    with torch.cuda.device(0):\n",
    "        print(f\"Processing {model_name}\")\n",
    "        #skip model if already processed\n",
    "        MODEL_NAME = model_name\n",
    "        path_to_safe = f'/workspace/src/data/qrels_{modality}_pool_{MODEL_NAME.replace(\"/\", \"_\")}.json'\n",
    "\n",
    "        if os.path.exists(path_to_safe):\n",
    "            print(f\"Skipping {model_name} because it already exists\")\n",
    "            continue\n",
    "\n",
    "        judge_osllm = OSLLMJudge( model_name=model_name, few_shot_count=0, num_gpus=1, device=\"cuda\")\n",
    "        all_judgments = single_process_topics(topics, pool_documents, judge_osllm)\n",
    "\n",
    "        with open(path_to_safe, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(all_judgments, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"Auto qrels saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "\n",
    "    with torch.cuda.device(0):\n",
    "        print(f\"Processing {model_name}\")\n",
    "        #skip model if already processed\n",
    "        MODEL_NAME = model_name\n",
    "        path_to_safe = f'/workspace/src/data/qrels_{modality}_pool_{MODEL_NAME.replace(\"/\", \"_\")}.json'\n",
    "\n",
    "        if os.path.exists(path_to_safe):\n",
    "            print(f\"Skipping {model_name} because it already exists\")\n",
    "            continue\n",
    "\n",
    "        judge_osllm = OSLLMJudge( model_name=model_name, few_shot_count=0, num_gpus=1, device=\"cuda\")\n",
    "        all_judgments = single_process_topics(topics, pool_documents, judge_osllm)\n",
    "\n",
    "        with open(path_to_safe, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(all_judgments, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"Auto qrels saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
