{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# We save the model to a local mounted directory\n",
    "os.environ['HF_HOME'] = '/workspace/llm_models'\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "modality = \"passage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "from umbrela.gpt_judge import GPTJudge\n",
    "from umbrela.osllm_judge import OSLLMJudge \n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing.utils import parallel_process_topics, single_process_topics, load_vectorstore, load_pool_documents\n",
    "\n",
    "db_vals = dotenv_values(\"/workspace/src/.env\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_pool = json.load(open(\"/workspace/src/data/passage_pool.json\", \"r\"))\n",
    "table_pool = json.load(open(\"/workspace/src/data/table_pool.json\", \"r\"))\n",
    "\n",
    "passage_pool_for_rel = json.load(open(\"/workspace/src/data/passage_pool_for_rel.json\", \"r\"))\n",
    "\n",
    "topics = pickle.load(open(\"/workspace/src/data/topics.pkl\", \"rb\"))\n",
    "vectorstore = load_vectorstore(\"/workspace/src/preprocessing/vectorstores/chromadb_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modality == 'table':\n",
    "    pool_documents = load_pool_documents(table_pool, vectorstore)\n",
    "else:\n",
    "    pool_documents = load_pool_documents(passage_pool, vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"tiiuae/Falcon3-7B-Instruct\",\n",
    "               \"google/gemma-2-9b-it\",\n",
    "               \"meta-llama/Llama-3.2-3B-Instruct\",  \n",
    "               \"Qwen/Qwen2.5-14B-Instruct\", \n",
    "               \"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "               \"microsoft/phi-4\", \n",
    "               \"tiiuae/Falcon3-7B-Instruct\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(f\"Processing {model_name}\")\n",
    "    #skip model if already processed\n",
    "    MODEL_NAME = model_name\n",
    "    path_to_safe = f'/workspace/src/data/qrels_{modality}_pool_{MODEL_NAME.replace(\"/\", \"_\")}.json'\n",
    "\n",
    "    if os.path.exists(path_to_safe):\n",
    "        print(f\"Skipping {model_name} because it already exists\")\n",
    "        continue\n",
    "\n",
    "    judge_osllm = OSLLMJudge(\"cord19\", model_name, few_shot_count=0, num_gpus=1)\n",
    "    all_judgments = single_process_topics(topics, pool_documents, judge_osllm)\n",
    "\n",
    "    with open(path_to_safe, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(all_judgments, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Auto qrels saved\")\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
