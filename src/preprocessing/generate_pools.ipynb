{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get text chunks to index\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import langchain_core.documents\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from database.model import Base, Document, Table\n",
    "from database.chunk_model import Chunk_Base, Chunk\n",
    "\n",
    "from preprocessing.utils import create_vectorstore, load_vectorstore\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "db_vals = dotenv_values(\"/workspace/src/.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get text chunks to index\n",
    "\n",
    "chunk_engine = create_engine(f\"postgresql+psycopg2://{db_vals['USER']}:{db_vals['PASSWORD']}@{db_vals['ADDRESS']}:{db_vals['PORT']}/cord19chunks\", echo=False)\n",
    "chunk_session = Session(chunk_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_chunks = chunk_session.query(Chunk).filter(Chunk.modality_type == \"text\").limit(1000).all()\n",
    "#table_chunks = chunk_session.query(Chunk).filter(Chunk.modality_type == \"table\").limit(1000).all()\n",
    "#text_chunks = chunk_session.query(Chunk).filter(Chunk.modality_type == \"text\").all()\n",
    "\n",
    "#chunks = text_chunks + table_chunks\n",
    "\n",
    "chunk_ids = chunk_session.query(Chunk.id).distinct().all()\n",
    "chunk_ids = [id[0] for id in chunk_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "doi_full_text = {}\n",
    "\n",
    "#skip if vectorstore already exists\n",
    "if os.path.exists(\"/workspace/src/preprocessing/vectorstores/chromadb_store\"):\n",
    "    vectorstore = load_vectorstore(\"/workspace/src/preprocessing/vectorstores/chromadb_store\")\n",
    "else:\n",
    "    #create vectorstore with first chunk_dois\n",
    "    first_chunks = chunk_session.query(Chunk).filter(Chunk.id.in_(chunk_ids[:BATCH_SIZE])).all()\n",
    "    vectorstore = create_vectorstore(first_chunks)\n",
    "\n",
    "    for i in tqdm(range(BATCH_SIZE, len(chunk_ids), BATCH_SIZE), position=1, leave=True):\n",
    "        print(f\"Processing: {i}/{len(chunk_ids)}\", end=\"\\r\")\n",
    "        batch = chunk_ids[i:i + BATCH_SIZE]\n",
    "        docs = chunk_session.query(Chunk).filter(Chunk.id.in_(batch)).all()\n",
    "\n",
    "        lang_docs = [langchain_core.documents.Document(page_content=chunk.chunk_text, metadata={\"id\": chunk.id, \"doi\": chunk.doi, \"chunk_type\": chunk.chunk_type}) for chunk in docs]\n",
    "        vectorstore.add_documents(lang_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_chunks = chunk_session.query(Chunk).filter(Chunk.modality_type == \"table\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pickle.load(open(\"/workspace/src/data/topics.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generade indices\n",
    "pooling_results = {}\n",
    "for query_id, query_info in tqdm(topics.items()):\n",
    "\n",
    "    query_text = query_info[\"title\"] + \" \" + query_info[\"description\"]\n",
    "\n",
    "    results = vectorstore.similarity_search(query_text, k=100)\n",
    "    pooling_results[query_id] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pooling_results, open(\"/workspace/src/data/pooling_results_cosine.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate bm25 pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_docs = [langchain_core.documents.Document(page_content=chunk.chunk_text, metadata={\"id\": chunk.id, \"doi\": chunk.doi, \"chunk_type\": chunk.chunk_type}) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_bm25 = BM25Retriever.from_documents(lang_docs, preprocess_func=word_tokenize, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_results_bm25 = {}\n",
    "for query_id, query_info in tqdm(topics.items()):\n",
    "\n",
    "    query_text = query_info[\"title\"] + \" \" + query_info[\"description\"]\n",
    "\n",
    "    results = retriever_bm25.invoke(query_text, k=100)\n",
    "    pooling_results_bm25[query_id] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pooling_results_bm25, open(\"/workspace/src/data/pooling_results_bm25.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
