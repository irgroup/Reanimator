{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/src/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first get dois and corresponding full_text\n",
    "\n",
    "\n",
    "import ir_datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from database.model import Base, Document, Table\n",
    "from database.chunk_model import Chunk_Base, Chunk\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import langchain_core.documents\n",
    "\n",
    "db_vals = dotenv_values(\"/workspace/src/.env\")\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"postgresql+psycopg2://{db_vals['USER']}:{db_vals['PASSWORD']}@{db_vals['ADDRESS']}:{db_vals['PORT']}/{db_vals['DB']}\", echo=False)\n",
    "session = Session(engine)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dois = list(set(doi[0] for doi in session.query(Document.doi).all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "doi_full_text = {}\n",
    "\n",
    "for i in tqdm(range(0, len(all_dois), BATCH_SIZE)):\n",
    "    batch = all_dois[i:i + BATCH_SIZE]\n",
    "    docs = session.query(Document).filter(Document.doi.in_(batch)).all()\n",
    "    for doc in docs:\n",
    "        doi_full_text[doc.doi] = (doc.title, doc.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = ir_datasets.load(\"cord19/fulltext/trec-covid\")\n",
    "#docs = {}\n",
    "#\n",
    "#for doc in tqdm(dataset.docs_iter()):\n",
    "#    if doc.doi in all_dois:\n",
    "#        docs[doc.doi] = {\"title\": doc.title, \"abstract\": doc.abstract, \"body\" : doc.body}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_full_text(doc):\n",
    "    full_text = \"\"\n",
    "    full_text += f\"{doc['title']} \\n\"\n",
    "    full_text += f\"{doc['abstract']} \\n\"\n",
    "\n",
    "    for section in doc[\"body\"]:\n",
    "        full_text += f\"{section.title} \\n\" \n",
    "        full_text += f\"{section.text} \\n\" \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_full_text_docling(row):\n",
    "    return f\"Title: {row[0]} \\n\\n {row[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = {}\n",
    "for doi, doc in tqdm(doi_full_text.items()):\n",
    "    full_texts[doi] = gen_full_text_docling(doi_full_text[doi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas, texts = [doi for doi in sorted(full_texts.keys())], [full_texts[doi] for doi in sorted(full_texts.keys())]\n",
    "print(len(metadatas), len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,  # chunk size (characters)\n",
    "    chunk_overlap=100,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_docs = [langchain_core.documents.Document(page_content=text, metadata={\"doi\": doi}) for doi, text in full_texts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_documents(lang_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_engine = create_engine(f\"postgresql+psycopg2://{db_vals['USER']}:{db_vals['PASSWORD']}@{db_vals['ADDRESS']}:{db_vals['PORT']}/cord19chunks\", echo=False)\n",
    "chunk_session = Session(chunk_engine)\n",
    "Chunk_Base.metadata.create_all(chunk_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add splits to database\n",
    "cnt = 0\n",
    "for split in tqdm(all_splits):\n",
    "    chunk = Chunk(doi=split.metadata[\"doi\"], chunk_text=split.page_content, chunk_type=\"RCTS_512_100\", modality_type=\"text\")\n",
    "    chunk_session.add(chunk)\n",
    "    cnt += 1\n",
    "    if cnt % 1000 == 0:\n",
    "        chunk_session.commit()\n",
    "chunk_session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_table_dois = list(set(doi[0] for doi in session.query(Table.ir_tab_id).all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [01:10<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "doi_table = {}\n",
    "\n",
    "for i in tqdm(range(0, len(all_table_dois), BATCH_SIZE)):\n",
    "    batch = all_table_dois[i:i + BATCH_SIZE]\n",
    "    docs = session.query(Table).filter(Table.ir_tab_id.in_(batch)).all()\n",
    "    for doc in docs:\n",
    "        doi_table[doc.ir_tab_id] = (doc.table_name, doc.header, doc.content, doc.caption, doc.references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_table(row):\n",
    "    table = \"\"\n",
    "    table += f\"Table Name: {row[0]} \\n\"\n",
    "    table += f\"Header: {row[1]} \\n\"\n",
    "    table += f\"Content: {row[2]} \\n\"\n",
    "    table += f\"Caption: {row[3]} \\n\"\n",
    "    table += f\"References: {row[4]} \\n\"\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144202/144202 [00:02<00:00, 54034.14it/s]\n"
     ]
    }
   ],
   "source": [
    "full_tables = {}\n",
    "for doi, doc in tqdm(doi_table.items()):\n",
    "    full_tables[doi] = gen_table(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=8192,  # chunk size (characters)\n",
    "    chunk_overlap=1000,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144202 144202\n"
     ]
    }
   ],
   "source": [
    "metadatas_t, texts_t = [doi for doi in sorted(full_tables.keys())], [full_tables[doi] for doi in sorted(full_tables.keys())]\n",
    "print(len(metadatas_t), len(texts_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_tables= [langchain_core.documents.Document(page_content=text, metadata={\"doi\": doi}) for doi, text in full_tables.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144202it [00:20, 7157.12it/s]\n"
     ]
    }
   ],
   "source": [
    "all_table_splits = table_splitter.split_documents(lang_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152861/152861 [01:36<00:00, 1576.53it/s]\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for split in tqdm(all_table_splits):\n",
    "    chunk = Chunk(doi=split.metadata[\"doi\"], chunk_text=split.page_content, chunk_type=\"RCTS_8192_1000\", modality_type=\"table\")\n",
    "    chunk_session.add(chunk)\n",
    "    cnt += 1\n",
    "    if cnt % 1000 == 0: \n",
    "        chunk_session.commit()\n",
    "chunk_session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk_session.query(Chunk).filter(Chunk.modality_type == \"table\").delete(synchronize_session=False)\n",
    "#chunk_session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
