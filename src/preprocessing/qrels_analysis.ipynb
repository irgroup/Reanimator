{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = \"table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_map = {\"has nothing to do with the query\" : 0, \n",
    "            \"related to the query but does not answer it\" : 1, \n",
    "            \"has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information\" : 2, \n",
    "            \"dedicated to the query and contains the exact answer\" : 3}\n",
    "\n",
    "def load_human_qrels(path):\n",
    "    model_id = path.split(\"log\")[-1].replace(\".json\", \"\")\n",
    "    model_name = f\"rater_{model_id}\"\n",
    "    qrels = json.load(open(path))\n",
    "    normed_qrels = defaultdict(list)\n",
    "    for assessment in qrels:\n",
    "        entry = {\"model\" : model_name, \"judgment\" : rel_map[assessment['relevance']], \"docid\" : int(assessment['id'])}\n",
    "        normed_qrels[assessment['topic']].append(entry)\n",
    "    return model_name, dict(normed_qrels)\n",
    "\n",
    "def load_machine_qrels(path):\n",
    "    model_name = path.split(\"pool_\")[-1].replace(\".json\", \"\") \n",
    "    qrels = json.load(open(path))\n",
    "    return model_name, qrels\n",
    "    \n",
    "    \n",
    "\n",
    "def make_qrels_comparable(qrels_a, qrels_b):\n",
    "    topic_ids_a = {topic:[ele['docid'] for ele in qrels_a[topic]] for topic in qrels_a.keys()} \n",
    "    topic_ids_b = {topic:[ele['docid'] for ele in qrels_b[topic]] for topic in qrels_b.keys()}\n",
    "    topic_id_overlap = {k: list(set(topic_ids_a[k]) & set(topic_ids_b[k])) for k in topic_ids_a.keys()}\n",
    "    \n",
    "    new_qrels_a = {}\n",
    "    new_qrels_b = {}\n",
    "    for key in topic_id_overlap.keys():\n",
    "        new_qrels_a[key] = [ele for ele in qrels_a[key] if ele['docid'] in topic_id_overlap[key]]\n",
    "        new_qrels_b[key] = [ele for ele in qrels_b[key] if ele['docid'] in topic_id_overlap[key]]\n",
    "    return new_qrels_a, new_qrels_b\n",
    "\n",
    "def check_res_status(qrel_data, verbose=False):\n",
    "    sanity_cnt = 0\n",
    "    bad_cnt = 0\n",
    "    for query_id, query_data in tqdm(qrel_data.items()):\n",
    "        for assessment in query_data:\n",
    "            if assessment['result_status'] == 1:\n",
    "                sanity_cnt += 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(assessment)\n",
    "                bad_cnt += 1\n",
    "    print(f\"Sanity count: {sanity_cnt}, Bad count: {bad_cnt}\")\n",
    "    print(f\"Coverage: {(sanity_cnt + bad_cnt) / 10000} \")\n",
    "\n",
    "def eval_status(qrel_paths):\n",
    "    for qrel_path in qrel_paths:\n",
    "        try:    \n",
    "            qrel_data = json.load(open(qrel_path))\n",
    "            print(qrel_path)\n",
    "            check_res_status(qrel_data)\n",
    "            print(\"-\"*100)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {qrel_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "def flatten_ratings(ratings_dict):\n",
    "    \"\"\"\n",
    "    Converts a nested ratings dictionary with the structure:\n",
    "    \n",
    "        {\n",
    "          'topic1': [\n",
    "             {'model': 'rater_X', 'judgment': int, 'docid': int}, \n",
    "             ...\n",
    "          ],\n",
    "          'topic2': [...],\n",
    "          ...\n",
    "        }\n",
    "    \n",
    "    into a flat dictionary mapping (topic, docid) -> judgment.\n",
    "    This way, if the same docid appears in different topics, they are treated\n",
    "    as separate rating items.\n",
    "    \"\"\"\n",
    "    flattened = {}\n",
    "    for topic, rating_list in ratings_dict.items():\n",
    "        for entry in rating_list:\n",
    "            # Use (topic, docid) as the key to avoid overriding entries\n",
    "            key = (topic, entry['docid'])\n",
    "            flattened[key] = entry['judgment']\n",
    "    return flattened\n",
    "\n",
    "def compute_cohens_kappa(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Computes Cohen's Kappa between two dictionaries with nested rating entries.\n",
    "    Ratings are aligned based on the composite key (topic, docid).\n",
    "    \"\"\"\n",
    "    # Flatten the dictionaries using the composite key (topic, docid)\n",
    "    ratings1 = flatten_ratings(dict1)\n",
    "    ratings2 = flatten_ratings(dict2)\n",
    "    \n",
    "    # Find common (topic, docid) pairs to compare.\n",
    "    common_keys = set(ratings1.keys()).intersection(ratings2.keys())\n",
    "    if not common_keys:\n",
    "        #skip if no overlapping (topic, docid) pairs between the two dictionaries.  \n",
    "        return 0\n",
    "        #raise ValueError(\"No overlapping (topic, docid) pairs between the two dictionaries.\")\n",
    "    \n",
    "    # Create parallel lists of judgments for these common keys.\n",
    "    judgments1 = [ratings1[key] for key in common_keys]\n",
    "    judgments2 = [ratings2[key] for key in common_keys]\n",
    "    \n",
    "    # Compute and return Cohen's Kappa.\n",
    "    kappa = cohen_kappa_score(judgments1, judgments2)\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modality == \"passage\":\n",
    "    human_qrels_paths = glob.glob(f\"/workspace/src/data/human_qrels/*_chunk*\")\n",
    "    machine_qrels_paths = glob.glob(f\"/workspace/src/data/qrels_passage_pool*\")  \n",
    "\n",
    "elif modality == \"table\":\n",
    "    human_qrels_paths = glob.glob(f\"/workspace/src/data/human_qrels/*_table*\")\n",
    "    machine_qrels_paths = glob.glob(f\"/workspace/src/data/qrels_table_pool*\")\n",
    "\n",
    "\n",
    "human_qrels = {}\n",
    "for path in human_qrels_paths:\n",
    "    name, qrels = load_human_qrels(path)\n",
    "    human_qrels[name] = qrels\n",
    "\n",
    "machine_qrels = {}\n",
    "for path in machine_qrels_paths:\n",
    "    name, qrels = load_machine_qrels(path)\n",
    "    machine_qrels[name] = qrels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['o3-mini-2025-01-31', 'gpt-4o-2024-11-20', 'gpt-4o-mini-2024-07-18', 'meta-llama_Llama-3.2-3B-Instruct', 'mistralai_Mistral-Small-Instruct-2409', 'mistralai_Mistral-7B-Instruct-v0.3', 'microsoft_phi-4', 'google_gemma-2-9b-it', 'tiiuae_Falcon3-7B-Instruct', 'Qwen_Qwen2.5-14B-Instruct'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_qrels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_token_cost(judgments, model_name, input_cost = 0.15, output_cost = 0.6,reasoning_const=100):\n",
    "    encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "    input_text = \"\"\n",
    "    output_text = \"\"\n",
    "    judgments_count = 0\n",
    "\n",
    "    for topic, item_list in judgments.items():\n",
    "        for item in item_list:\n",
    "            input_text += item['prompt'] + \"\\n\"\n",
    "            output_text += item['prediction'] + \"\\n\"\n",
    "            judgments_count += 1\n",
    "    input_tokens = encoding.encode(input_text)\n",
    "    output_tokens = encoding.encode(output_text)\n",
    "\n",
    "\n",
    "    input_cost = len(input_tokens) * input_cost / 1000**2\n",
    "    output_cost = len(output_tokens) * output_cost / 1000**2\n",
    "    print(len(input_tokens))\n",
    "    if \"o3\" in model_name:\n",
    "        input_cost = (len(input_tokens)+(reasoning_const*judgments_count)) * input_cost / 1000**2\n",
    "\n",
    "    print(f\"Input cost: {input_cost}, Output cost: {output_cost}\")\n",
    "    print(f\"Total cost for {model_name}: {input_cost + output_cost}$\")\n",
    "    print(f\"Cost per judgment: {round((input_cost + output_cost) / judgments_count, 6)}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_inter_rater_analysis_df(modality, human_qrels, machine_qrels, filter_machine = []):\n",
    "    # Create an empty list to collect rows of data. \n",
    "    rows = []\n",
    "    # Loop over each human rater and machine system in sorted order.\n",
    "    for human in sorted(human_qrels.keys()):\n",
    "        for machine in sorted(machine_qrels.keys()):\n",
    "            kappa = compute_cohens_kappa(human_qrels[human], machine_qrels[machine])\n",
    "\n",
    "            #prevent machine human pair is aleady processed\n",
    "            reverse_rater_list = [row for row in rows if row['Human Rater'] == machine and row['Machine System'] == human]\n",
    "            if reverse_rater_list:\n",
    "                continue\n",
    "\n",
    "            if machine in filter_machine:\n",
    "                continue\n",
    "            \n",
    "            if kappa != 0 and kappa != 1:\n",
    "                rows.append({\n",
    "                    'Human Rater': human,\n",
    "                    'Machine System': machine,\n",
    "                    \"Cohen's Kappa\": kappa\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame from the list of dictionaries.\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Format the \"Cohen's Kappa\" column to display 4 decimal places.\n",
    "    df[\"Cohen's Kappa\"] = df[\"Cohen's Kappa\"].map(lambda x: f'{x:.4f}')\n",
    "    \n",
    "    # Print the title and the DataFrame.\n",
    "    print(f\"Inter-Rater Analysis of {modality} Qrels:\\n\")\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have defined human_qrels, machine_qrels, and compute_cohens_kappa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "filter_machine = [\"meta-llama_Llama-3.2-3B-Instruct\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-Rater Analysis of passage Qrels:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = print_inter_rater_analysis_df(modality, human_qrels, machine_qrels, filter_machine)\n",
    "df[\"Cohen's Kappa\"] = pd.to_numeric(df[\"Cohen's Kappa\"], errors=\"coerce\")\n",
    "\n",
    "# Group by \"Machine System\" and compute the mean Cohen's Kappa (ignoring NaN values)\n",
    "avg_kappa = df.groupby(\"Machine System\")[\"Cohen's Kappa\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine System\n",
       "o3-mini-2025-01-31                       0.341600\n",
       "gpt-4o-mini-2024-07-18                   0.336962\n",
       "tiiuae_Falcon3-7B-Instruct               0.324875\n",
       "Qwen_Qwen2.5-14B-Instruct                0.319250\n",
       "gpt-4o-2024-11-20                        0.288863\n",
       "google_gemma-2-9b-it                     0.279825\n",
       "microsoft_phi-4                          0.217762\n",
       "mistralai_Mistral-Small-Instruct-2409    0.182375\n",
       "mistralai_Mistral-7B-Instruct-v0.3       0.155438\n",
       "Name: Cohen's Kappa, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-Rater Analysis of passage Qrels:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human Rater</th>\n",
       "      <th>Machine System</th>\n",
       "      <th>Cohen's Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rater_1</td>\n",
       "      <td>rater_2</td>\n",
       "      <td>0.2346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rater_3</td>\n",
       "      <td>rater_4</td>\n",
       "      <td>0.5457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rater_5</td>\n",
       "      <td>rater_6</td>\n",
       "      <td>0.4250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rater_7</td>\n",
       "      <td>rater_8</td>\n",
       "      <td>0.2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Human Rater Machine System Cohen's Kappa\n",
       "0     rater_1        rater_2        0.2346\n",
       "1     rater_3        rater_4        0.5457\n",
       "2     rater_5        rater_6        0.4250\n",
       "3     rater_7        rater_8        0.2004"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_inter_rater_analysis_df(modality, human_qrels, human_qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
