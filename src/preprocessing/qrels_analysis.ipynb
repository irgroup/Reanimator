{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import copy\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = \"passage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_map = {\"has nothing to do with the query\" : 0, \n",
    "            \"related to the query but does not answer it\" : 1, \n",
    "            \"has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information\" : 2, \n",
    "            \"dedicated to the query and contains the exact answer\" : 3}\n",
    "\n",
    "def load_human_qrels(path):\n",
    "    model_id = path.split(\"log\")[-1].replace(\".json\", \"\")\n",
    "    model_name = f\"rater_{model_id}\"\n",
    "    qrels = json.load(open(path))\n",
    "    normed_qrels = defaultdict(list)\n",
    "    for assessment in qrels:\n",
    "        entry = {\"model\" : model_name, \"judgment\" : rel_map[assessment['relevance']], \"docid\" : int(assessment['id'])}\n",
    "        normed_qrels[assessment['topic']].append(entry)\n",
    "    return model_name, dict(normed_qrels)\n",
    "\n",
    "def load_machine_qrels(path):\n",
    "    model_name = path.split(\"pool_\")[-1].replace(\".json\", \"\") \n",
    "    qrels = json.load(open(path))\n",
    "    return model_name, qrels\n",
    "    \n",
    "    \n",
    "\n",
    "def make_qrels_comparable(qrels_a, qrels_b):\n",
    "    topic_ids_a = {topic:[ele['docid'] for ele in qrels_a[topic]] for topic in qrels_a.keys()} \n",
    "    topic_ids_b = {topic:[ele['docid'] for ele in qrels_b[topic]] for topic in qrels_b.keys()}\n",
    "    topic_id_overlap = {k: list(set(topic_ids_a[k]) & set(topic_ids_b[k])) for k in topic_ids_a.keys()}\n",
    "    \n",
    "    new_qrels_a = {}\n",
    "    new_qrels_b = {}\n",
    "    for key in topic_id_overlap.keys():\n",
    "        new_qrels_a[key] = [ele for ele in qrels_a[key] if ele['docid'] in topic_id_overlap[key]]\n",
    "        new_qrels_b[key] = [ele for ele in qrels_b[key] if ele['docid'] in topic_id_overlap[key]]\n",
    "    return new_qrels_a, new_qrels_b\n",
    "\n",
    "def check_res_status(qrel_data, verbose=False):\n",
    "    sanity_cnt = 0\n",
    "    bad_cnt = 0\n",
    "    for query_id, query_data in tqdm(qrel_data.items()):\n",
    "        for assessment in query_data:\n",
    "            if assessment['result_status'] == 1:\n",
    "                sanity_cnt += 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(assessment)\n",
    "                bad_cnt += 1\n",
    "    print(f\"Sanity count: {sanity_cnt}, Bad count: {bad_cnt}\")\n",
    "    print(f\"Coverage: {(sanity_cnt + bad_cnt) / 10000} \")\n",
    "\n",
    "def eval_status(qrel_paths):\n",
    "    for qrel_path in qrel_paths:\n",
    "        try:    \n",
    "            qrel_data = json.load(open(qrel_path))\n",
    "            print(qrel_path)\n",
    "            check_res_status(qrel_data)\n",
    "            print(\"-\"*100)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {qrel_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "def flatten_ratings(ratings_dict):\n",
    "    \"\"\"\n",
    "    Converts a nested ratings dictionary with the structure:\n",
    "    \n",
    "        {\n",
    "          'topic1': [\n",
    "             {'model': 'rater_X', 'judgment': int, 'docid': int}, \n",
    "             ...\n",
    "          ],\n",
    "          'topic2': [...],\n",
    "          ...\n",
    "        }\n",
    "    \n",
    "    into a flat dictionary mapping (topic, docid) -> judgment.\n",
    "    This way, if the same docid appears in different topics, they are treated\n",
    "    as separate rating items.\n",
    "    \"\"\"\n",
    "    flattened = {}\n",
    "    for topic, rating_list in ratings_dict.items():\n",
    "        for entry in rating_list:\n",
    "            # Use (topic, docid) as the key to avoid overriding entries\n",
    "            key = (topic, entry['docid'])\n",
    "            flattened[key] = entry['judgment']\n",
    "    return flattened\n",
    "\n",
    "def compute_cohens_kappa(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Computes Cohen's Kappa between two dictionaries with nested rating entries.\n",
    "    Ratings are aligned based on the composite key (topic, docid).\n",
    "    \"\"\"\n",
    "    # Flatten the dictionaries using the composite key (topic, docid)\n",
    "    ratings1 = flatten_ratings(dict1)\n",
    "    ratings2 = flatten_ratings(dict2)\n",
    "    print(ratings1)\n",
    "    # Find common (topic, docid) pairs to compare.\n",
    "    common_keys = set(ratings1.keys()).intersection(ratings2.keys())\n",
    "    if not common_keys:\n",
    "        #skip if no overlapping (topic, docid) pairs between the two dictionaries.  \n",
    "        return 0\n",
    "        #raise ValueError(\"No overlapping (topic, docid) pairs between the two dictionaries.\")\n",
    "    \n",
    "    # Create parallel lists of judgments for these common keys.\n",
    "    judgments1 = [ratings1[key] for key in common_keys]\n",
    "    judgments2 = [ratings2[key] for key in common_keys]\n",
    "    \n",
    "    # Compute and return Cohen's Kappa.\n",
    "    kappa = cohen_kappa_score(judgments1, judgments2)\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modality == \"passage\":\n",
    "    human_qrels_paths = glob.glob(f\"/workspace/src/data/human_qrels/*_chunk*\")\n",
    "    machine_qrels_paths = glob.glob(f\"/workspace/src/data/qrels_passage_pool*\")  \n",
    "\n",
    "elif modality == \"table\":\n",
    "    human_qrels_paths = glob.glob(f\"/workspace/src/data/human_qrels/*_table*\")\n",
    "    machine_qrels_paths = glob.glob(f\"/workspace/src/data/qrels_table_pool*\")\n",
    "\n",
    "\n",
    "human_qrels = {}\n",
    "for path in human_qrels_paths:\n",
    "    name, qrels = load_human_qrels(path)\n",
    "    human_qrels[name] = qrels\n",
    "\n",
    "machine_qrels = {}\n",
    "for path in machine_qrels_paths:\n",
    "    name, qrels = load_machine_qrels(path)\n",
    "    machine_qrels[name] = qrels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['meta-llama_Llama-3.2-3B-Instruct', 'Qwen_Qwen2.5-14B-Instruct', 'mistralai_Mistral-7B-Instruct-v0.3', 'gpt-4o-2024-11-20', 'majority_vote', 'gpt-4o-mini-2024-07-18', 'o3-mini-2025-01-31', 'microsoft_phi-4', 'tiiuae_Falcon3-7B-Instruct', 'google_gemma-2-9b-it', 'mistralai_Mistral-Small-Instruct-2409'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_qrels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_token_cost(judgments, model_name, input_cost = 0.15, output_cost = 0.6,reasoning_const=100):\n",
    "    encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "    input_text = \"\"\n",
    "    output_text = \"\"\n",
    "    judgments_count = 0\n",
    "\n",
    "    for topic, item_list in judgments.items():\n",
    "        for item in item_list:\n",
    "            input_text += item['prompt'] + \"\\n\"\n",
    "            output_text += item['prediction'] + \"\\n\"\n",
    "            judgments_count += 1\n",
    "    input_tokens = encoding.encode(input_text)\n",
    "    output_tokens = encoding.encode(output_text)\n",
    "\n",
    "\n",
    "    input_cost = len(input_tokens) * input_cost / 1000**2\n",
    "    output_cost = len(output_tokens) * output_cost / 1000**2\n",
    "    print(len(input_tokens))\n",
    "    if \"o3\" in model_name:\n",
    "        input_cost = (len(input_tokens)+(reasoning_const*judgments_count)) * input_cost / 1000**2\n",
    "\n",
    "    print(f\"Input cost: {input_cost}, Output cost: {output_cost}\")\n",
    "    print(f\"Total cost for {model_name}: {input_cost + output_cost}$\")\n",
    "    print(f\"Cost per judgment: {round((input_cost + output_cost) / judgments_count, 6)}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_inter_rater_analysis_df(modality, human_qrels, machine_qrels, filter_machine = []):\n",
    "    # Create an empty list to collect rows of data. \n",
    "    rows = []\n",
    "    # Loop over each human rater and machine system in sorted order.\n",
    "    for human in sorted(human_qrels.keys()):\n",
    "        for machine in sorted(machine_qrels.keys()):\n",
    "            kappa = compute_cohens_kappa(human_qrels[human], machine_qrels[machine])\n",
    "            #prevent machine human pair is aleady processed\n",
    "            reverse_rater_list = [row for row in rows if row['Human Rater'] == machine and row['Machine System'] == human]\n",
    "            if reverse_rater_list:\n",
    "                continue\n",
    "\n",
    "            if machine in filter_machine:\n",
    "                continue\n",
    "            \n",
    "            if kappa != 0 and kappa != 1:\n",
    "                rows.append({\n",
    "                    'Human Rater': human,\n",
    "                    'Machine System': machine,\n",
    "                    \"Cohen's Kappa\": kappa\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame from the list of dictionaries.\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Format the \"Cohen's Kappa\" column to display 4 decimal places.\n",
    "    df[\"Cohen's Kappa\"] = df[\"Cohen's Kappa\"].map(lambda x: f'{x:.4f}')\n",
    "    \n",
    "    # Print the title and the DataFrame.\n",
    "    print(f\"Inter-Rater Analysis of {modality} Qrels:\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "filter_machine = [\"meta-llama_Llama-3.2-3B-Instruct\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_qrels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = print_inter_rater_analysis_df(modality, human_qrels, machine_qrels, filter_machine=[\"meta-llama_Llama-3.2-3B-Instruct\"])\n",
    "df[\"Cohen's Kappa\"] = pd.to_numeric(df[\"Cohen's Kappa\"], errors=\"coerce\")\n",
    "\n",
    "# Group by \"Machine System\" and compute the mean Cohen's Kappa (ignoring NaN values)\n",
    "avg_kappa = df.groupby(\"Machine System\")[\"Cohen's Kappa\"].mean().sort_values(ascending=False)\n",
    "avg_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_inter_rater_analysis_df(modality, human_qrels, human_qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate majority vote qrels\n",
    "gpt_qrels_passage_paths = sorted([path for path in glob.glob(f\"/workspace/src/data/qrels_{modality}*\") if \"gpt\" in path or \"o3\" in path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_qrels_passage= [load_machine_qrels(path)[1] for path in gpt_qrels_passage_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_qrels_passage_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(lst):\n",
    "    \"\"\"\n",
    "    Given a list of numbers, return the majority vote.\n",
    "    If there is a tie, return the candidate closest to the average.\n",
    "    \"\"\"\n",
    "    counts = Counter(lst)\n",
    "    max_count = max(counts.values())\n",
    "    # Get all candidates with the maximum count.\n",
    "    candidates = [num for num, count in counts.items() if count == max_count]\n",
    "    \n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    else:\n",
    "        # In case of tie, calculate the average of the list.\n",
    "        avg = sum(lst) / len(lst)\n",
    "        # Return the candidate that is closest to the average.\n",
    "        return min(candidates, key=lambda x: abs(x - avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_majority_vote_qrels(gpt_qrels_table):\n",
    "    vote_list = [{k:[gpt_qrels_table[j][k][i]['judgment'] for i in range(len(gpt_qrels_table[j][k]))] for k in list(gpt_qrels_table[j].keys())} for j in range(len(gpt_qrels_table))]\n",
    "    all_votes = {}\n",
    "    for key in vote_list[0]:\n",
    "        k = len(vote_list[0][key])\n",
    "        # For each index in the inner list, collect the elements from all dictionaries.\n",
    "        aggregated = [[d[key][i] for d in vote_list] for i in range(k)]\n",
    "        all_votes[key] = aggregated\n",
    "\n",
    "    majority_results = {}\n",
    "    for key, list_of_votes in all_votes.items():\n",
    "        majority_results[key] = [majority_vote(votes) for votes in list_of_votes]\n",
    "\n",
    "    dummy_qrel = copy.deepcopy(gpt_qrels_table[0])\n",
    "    \n",
    "    for topic, items in majority_results.items():\n",
    "        for i in range(len(items)):\n",
    "            dummy_qrel[topic][i]['judgment'] = items[i]\n",
    "            dummy_qrel[topic][i]['model'] = \"majority_vote\"\n",
    "            dummy_qrel[topic][i]['prediction'] = \"-\"\n",
    "            dummy_qrel[topic][i]['result_status'] = 1\n",
    "\n",
    "    return dummy_qrel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_qrel = generate_majority_vote_qrels(gpt_qrels_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe majority vote qrels to json\n",
    "with open(f\"/workspace/src/data/qrels_{modality}_pool_majority_vote.json\", \"w\") as f:\n",
    "    json.dump(majority_vote_qrel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
