{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# We save the model to a local mounted directory\n",
    "os.environ['HF_HOME'] = '/workspace/llm_models'\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "modality = \"passage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "from umbrela.gpt_judge import GPTJudge\n",
    "from umbrela.osllm_judge import OSLLMJudge \n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing.utils import parallel_process_topics, single_process_topics, load_vectorstore, load_pool_documents, single_process_topics_safe,process_topic\n",
    "\n",
    "db_vals = dotenv_values(\"/workspace/src/.env\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/preprocessing/utils.py:87: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings(model=embedding_model)\n",
      "/workspace/src/preprocessing/utils.py:88: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "passage_pool = json.load(open(\"/workspace/src/data/passage_pool.json\", \"r\"))\n",
    "table_pool = json.load(open(\"/workspace/src/data/table_pool.json\", \"r\"))\n",
    "\n",
    "passage_pool_for_rel = json.load(open(\"/workspace/src/data/passage_pool_for_rel.json\", \"r\"))\n",
    "\n",
    "topics = pickle.load(open(\"/workspace/src/data/topics.pkl\", \"rb\"))\n",
    "vectorstore = load_vectorstore(\"/workspace/src/preprocessing/vectorstores/chromadb_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modality == 'table':\n",
    "    pool_documents = load_pool_documents(table_pool, vectorstore)\n",
    "else:\n",
    "    pool_documents = load_pool_documents(passage_pool, vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"mistralai/Mistral-Small-Instruct-2409\",\n",
    "                \"microsoft/phi-4\",\n",
    "                \"tiiuae/Falcon3-7B-Instruct\", \n",
    "               \"Qwen/Qwen2.5-14B-Instruct\", \n",
    "               \"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "               \"google/gemma-2-9b-it\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"microsoft/phi-4\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_classic_chars(text):\n",
    "    return text.replace(\",\", \" \").replace(\"'\", \" \").replace(\".\", \" \").replace(\"$\", \" \").replace(\"^\", \" \").replace(\"≥\", \" \").replace(\"°\", \" \").replace(\"×\", \" \").replace(\"μ\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_uncommon_characters(s: str, common_chars: str = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\") -> str:\n",
    "    \"\"\"\n",
    "    Replace every character in the string `s` that is not in `common_chars` with a space.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string.\n",
    "        common_chars (str): A string containing all characters considered \"common\".\n",
    "                            By default, this is all alphanumeric characters.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified string with uncommon characters replaced by spaces.\n",
    "    \"\"\"\n",
    "    return ''.join(c if c in common_chars else ' ' for c in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing microsoft/phi-4\n",
      "Warning!! Prompt file expects input fields namely: (examples, query, passage).\n",
      "Loading model...\n",
      "Load Model {'torch_dtype': torch.float16, 'revision': 'main'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a473c227b1445cbd9aab944a6223d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:46<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response to `it caused was named Coronavirus Disease 2019 (COVID-19) 21 . While the origin of the virus has been related to SARS-like CoVs circulating in bat populations, an intermediate host between bats and humans is yet to be determined 22 . Multiple phylogenetic studies have also served as an approach for hypothesis in the animal origins of SARS-CoV-2 15,16 . At the moment, multiple animals are a matter of investigation as possible susceptible hosts for SARS-CoV-2 23 .` & `coronavirus origin what is the origin of COVID-19`: ##m: ##t: ##o:\n",
      "Invalid response to `4 | COVID -19\n",
      "\n",
      "4.1 | Origin of the virus\n",
      "\n",
      "The culprit of the recent pandemic in 2020, termed by the WHO as coronavirus disease -2019 (COVID -19), is yet another novel coronavirus; now named SARS -CoV -2. The ultimate origin of SARS -CoV -2 is believed to be bats with a possible unknown intermediate host (possibly pangolins) transmitting the virus to humans. 28,29\n",
      "\n",
      "4.2 | History of the disease` & `coronavirus origin what is the origin of COVID-19`: \n",
      "Invalid response to `How did COVID -19 originate? The source of COVID19 outbreak is yet to be determined. Although some preliminary investigations in China identified samples in the Huanan Seafood Wholesale Market of Wuhan city to be positive for SARS-CoV2, the zoonotic source of the outbreak is still ambiguous. All throughout the pandemic, it was argued by many researchers that bats which harbor a lot of viruses ( but remain unaffected by these ) are a reason for the COVID-19 outbreak. This was because some bats called` & `coronavirus origin what is the origin of COVID-19`: ##m: ##t: ##o:\n",
      "Invalid response to `What is the role of pharmacovigilance in the pandemic situation? More specifically, what is the role of the International Society of Pharmacovigilance (ISoP) during the current coronavirus 2019 (COVID-19) outbreak?` & `coronavirus origin what is the origin of COVID-19`: ##m: ##t: ##o:\n",
      "Invalid response to `SARS-CoV-2 is the recently discovered virus which has resulted in the pandemic illness known as coronavirus disease 2019 (COVID-19) [1]. It is from the family of viruses known as Coronaviridae, which were also responsible for severe acute respiratory syndrome (SARS), which originated in China in 2002, and Middle East respiratory syndrome coronavirus in 2012. It is thought to be a zoonosis, possibly originating in bats, which has undergone intraspecies transfer between intermediary hosts and humans.` & `coronavirus origin what is the origin of COVID-19`: ##m: ##t: ##o:\n",
      "Successfully processed and saved topic: 1\n",
      "Execution time for microsoft/phi-4: 235.62 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Specify the JSON file to store the execution times.\n",
    "output_file = f'execution_times_{modality}.json'\n",
    "\n",
    "# Dictionary to hold execution times per model.\n",
    "execution_times = {}\n",
    "number_of_samples = 50\n",
    "\n",
    "topics = {key:val for key, val in topics.items() if key == \"1\"}\n",
    "pool_documents = {key:val[:number_of_samples] for key, val in pool_documents.items()}\n",
    "\n",
    "for model_name in model_names:\n",
    "    with torch.cuda.device(1):\n",
    "        # Ensure that all CUDA operations are complete before starting the timer.\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        \n",
    "        print(f\"Processing {model_name}\")\n",
    "        MODEL_NAME = model_name\n",
    "        \n",
    "        judge_osllm = OSLLMJudge(\"cord19\", model_name, few_shot_count=0, num_gpus=1, device=\"cuda\")\n",
    "        start_time = time.time()\n",
    "        all_judgments = single_process_topics_safe(topics, pool_documents, judge_osllm, skip_existing=False)\n",
    "        \n",
    "        # Ensure that GPU operations are finished before stopping the timer.\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Execution time for {model_name}: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        # Save the time for the current model in the dictionary.\n",
    "        execution_times[model_name] = elapsed_time/number_of_samples\n",
    "        \n",
    "        # Write the updated dictionary to the JSON file.\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(execution_times, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "\n",
    "    with torch.cuda.device(1):\n",
    "        print(f\"Processing {model_name}\")\n",
    "        #skip model if already processed\n",
    "        MODEL_NAME = model_name\n",
    "        path_to_safe = f'/workspace/src/data/qrels_{modality}_pool_{MODEL_NAME.replace(\"/\", \"_\")}.json'\n",
    "\n",
    "        if os.path.exists(path_to_safe):\n",
    "            qlres = json.load(open(path_to_safe, \"r\"))\n",
    "            if len(qlres) == len(topics):\n",
    "                print(f\"Skipping {model_name} because it already exists\")\n",
    "                continue\n",
    "\n",
    "        judge_osllm = OSLLMJudge(\"cord19\", model_name, few_shot_count=0, num_gpus=1, device=\"cuda\")\n",
    "        all_judgments = single_process_topics_safe(topics, pool_documents, judge_osllm)\n",
    "\n",
    "        with open(path_to_safe, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(all_judgments, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"Auto qrels saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
